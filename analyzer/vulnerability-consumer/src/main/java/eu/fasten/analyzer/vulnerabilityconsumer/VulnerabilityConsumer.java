/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package eu.fasten.analyzer.vulnerabilityconsumer;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import eu.fasten.analyzer.vulnerabilityconsumer.db.MetadataUtility;
import eu.fasten.analyzer.vulnerabilityconsumer.utils.PURLPackage;
import eu.fasten.analyzer.vulnerabilityconsumer.utils.Vulnerability;
import eu.fasten.core.plugins.DBConnector;
import eu.fasten.core.plugins.KafkaPlugin;

import java.io.*;
import java.text.DateFormat;
import java.text.SimpleDateFormat;
import java.util.Collections;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Objects;
import java.util.Arrays;
import java.util.Optional;
import java.util.concurrent.atomic.*;
import java.util.stream.Collectors;

import org.apache.kafka.clients.producer.*;
import org.jooq.DSLContext;
import org.pf4j.Extension;
import org.pf4j.Plugin;
import org.pf4j.PluginWrapper;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class VulnerabilityConsumer extends Plugin {

    public VulnerabilityConsumer(PluginWrapper wrapper) {
        super(wrapper);
    }

    @Extension
    public static class VulnerabilityConsumerExtension implements KafkaPlugin, DBConnector {
        private String consumerTopic = "fasten.vulnerability.out";
        private String pendingTopic = consumerTopic + ".pending";
        private String outputPath;
        private static Map<String, DSLContext> contexts;
        private Exception pluginError = null;
        private String latestVulnJson = null;
        private ObjectMapper objectMapper = new ObjectMapper();
        private KafkaProducer<String, String> kafkaProducer;
        private static DateFormat sdf = new SimpleDateFormat("yyyy-MM-dd");
        private final static String[] extensions = new String[]{".java", ".py", ".pyw", ".c", ".cpp", ".h"};
        private final Logger logger = LoggerFactory.getLogger(VulnerabilityConsumerExtension.class.getName());

        @Override
        public void setDBConnection(Map<String, DSLContext> dslContexts) {
            this.contexts = dslContexts;
        }

        @Override
        public Optional<List<String>> consumeTopic() {
            return Optional.of(Collections.singletonList(consumerTopic));
        }

        @Override
        public void setTopic(String topicName) {
            this.consumerTopic = topicName;
            this.pendingTopic = topicName.endsWith(".pending") ? topicName : topicName + ".pending";
        }

        @Override
        public void consume(String record) {
            try {
                this.pluginError = null; // Reset Plugin error
                var metadataUtility = new MetadataUtility(); // Injecting for easier testing
                latestVulnJson = injectVulnerabilityIntoDB(record, metadataUtility);
            } catch (JsonProcessingException e) {
                logger.error("[ERROR] Could not parse vulnerability record JSON");
                setPluginError(e);
            }
        }

        @Override
        public Optional<String> produce() {
            if (latestVulnJson == null) {
                return Optional.empty();
            } else {
                return Optional.of(latestVulnJson);
            }
        }

        @Override
        public String getOutputPath() {
            return outputPath;
        }

        @Override
        public String name() {
            return "Vulnerability Consumer";
        }

        @Override
        public String description() {
            return "Reads vulnerabilities from Vulnerability Producer Kafka" +
                    " and injects them in the DB.";
        }

        @Override
        public String version() {
            return "0.0.1";
        }

        @Override
        public void start() {
        }

        @Override
        public void stop() {
        }

        @Override
        public Exception getPluginError() {
            return pluginError;
        }

        public void setPluginError(Exception throwable) {
            this.pluginError = throwable;
        }

        @Override
        public void freeResource() {
        }

        @Override
        public long getMaxConsumeTimeout() {
            return 3600000; // The VulnerabilityConsumer plugin takes up to 1h to process a record.
        }

        public void setKafkaProducer(KafkaProducer<String, String> kafkaProducer) {
            this.kafkaProducer = kafkaProducer;
        }

        /**
         * Method to inject the information contained in a Vulnerability Object.
         *
         * @param record          - Vulnerability JSON record
         * @param metadataUtility - Metadata DAO
         */
        public String injectVulnerabilityIntoDB(String record, MetadataUtility metadataUtility) throws JsonProcessingException {
            var v = objectMapper.readValue(record, Vulnerability.class);
            logger.info("[INFO] Read vulnerability " + v.getId() + " from Kafka");
            logger.info("[INFO] Injecting vulnerability " + v.getId() + " into the Database");
            var ecosystem = v.getPurls().size() > 0 ? getVulnerabilityEcosystem(v) : null;
            var context = ecosystem == null ? null : contexts.get(ecosystem);
            if (context == null) return objectMapper.writeValueAsString(v);
            v.setFirstPatchedPurls(v.getFirstPatchedPurls().stream()
                    .filter(Objects::nonNull).collect(Collectors.toList()));

            var pkgIds = metadataUtility.getPackageIds(context, v);
            if (pkgIds.size() == 0) return objectMapper.writeValueAsString(v);

            var pkgVersionIds = metadataUtility.getPackageVersionIds(v.getPurls(), context,
                    pkgIds, v);
            var pkgVersionPatchedIds = metadataUtility.getPackageVersionIds(v.getFirstPatchedPurls(),
                    context, pkgIds, null);

            if (pkgVersionIds.size() == 0) return objectMapper.writeValueAsString(v);
            var fastenUris = new HashSet<String>();
            var latestVersionId = pkgVersionIds.get(pkgVersionIds.size() - 1);
            pkgVersionPatchedIds.add(latestVersionId);

            checkFileExtensions(v);

            pkgVersionPatchedIds.forEach(pkgVersionId -> {
                v.getPatches().forEach(p -> {
                    logger.info("[INFO] Searching for callables in " + p.getFileName() + " for Package Version ID: " + pkgVersionId);
                    fastenUris.addAll(metadataUtility.getFastenUrisForPatch(p, pkgVersionId, context));
                });
            });

            logger.info("[INFO] Collected " + fastenUris.size() + " vulnerable fasten_uris ids");
            var pendingFlag = new AtomicBoolean(false);

            if (fastenUris.size() == 0) {
                pkgVersionIds.forEach(pkgVsnId -> {
                    if (metadataUtility.areCallablesMissing(pkgVsnId, context)) {
                        var vsn = metadataUtility.getCache().pkgVsnIdToVsn.get(pkgVsnId);
                        var pkgName = metadataUtility.getCache().pkgVsnToName.get(vsn);
                        var pkgCoord = ecosystem + "/packages/" + pkgName + "/" + vsn;
                        metadataUtility.sendIngestRequest(pkgCoord);
                        pendingFlag.set(true);
                    }
                });
            }

            var fullFastenUris = new HashSet<String>();
            var vulnCallableIds = new HashSet<Long>();
            var vulnPkgVersionIds = new HashSet<>(pkgVersionIds);
            fastenUris.forEach(uri -> {
                var callIds = metadataUtility.getCallableIdsForFastenUri(uri, vulnPkgVersionIds, context);
                callIds.forEach(id -> fullFastenUris.add(metadataUtility.getFullFastenUri(uri, id)));
                vulnCallableIds.addAll(callIds);
            });

            v.setFastenUris(fullFastenUris);
            writePatchDate(v);

            logger.info("[INFO] Injecting all the information in the DB");
            vulnPkgVersionIds.forEach(id -> metadataUtility.injectPackageVersionVulnerability(v, id, context));
            vulnCallableIds.forEach(id -> metadataUtility.injectCallableVulnerability(v, id, context));

            if (pendingFlag.get()) {
                logger.info("[INFO] Publishing vulnerability " + v.getId() + " to pending topic");
                kafkaProducer.send(new ProducerRecord<>(pendingTopic, record));
            }

            logger.info("[INFO] Setting output path to store the JSON statement for " + v.getId());
            outputPath = File.separator + "vuln" + File.separator + "consumer"
                    + File.separator + "statements" + File.separator + v.getId() + ".json";

            return objectMapper.writeValueAsString(v);
        }

        /**
         * Checks the extension of each file reported as Patch.
         * Supported extensions: {.java, .py, .c, .cpp}
         * @param v - Vulnerability Object
         */
        public static void checkFileExtensions(Vulnerability v) {
            var supportedPatches = new HashSet<Vulnerability.Patch>();
            v.getPatches().forEach(p -> {
                if (Arrays.stream(extensions).anyMatch(p.getFileName()::endsWith)) {
                    supportedPatches.add(p);
                }
            });
            v.setPatches(supportedPatches);
        }

        /**
         * Finds the correct DSLContext for the vulnerability given.
         *
         * @param v - Vulnerability Object
         * @return DSLContext
         */
        public String getVulnerabilityEcosystem(Vulnerability v) {
            assert v.getPurls().size() > 0;
            var purl = v.getPurls().iterator().next();
            try {
                var purlObj = PURLPackage.getObjectFromPurl(purl);
                return purlObj.getType();
            } catch (Exception e) {
                logger.error("PURL was not formatted correctly");
                return null;
            }
        }

        /**
         * Looks for the latest Patch Date and sets it
         *
         * @param v - Vulnerability Object
         */
        public static void writePatchDate(Vulnerability v) {
            if (v.getPatches().size() == 0) return;
            try {
                boolean set = false;
                var latestDate = sdf.parse("1950-01-01");
                for (Vulnerability.Patch patch : v.getPatches()) {
                    var patchDate = sdf.parse(patch.getPatchDate());
                    if (patchDate.after(latestDate)) {
                        latestDate = patchDate;
                        set = true;
                    }
                }
                if (set)
                    v.setPatchDate(sdf.format(latestDate));
            } catch (Exception e) {
            }
        }

        /**
         * Purge Vulnerability entries from the DB.
         */
        public void purgeVulnerabilitiesFromDB() {
            logger.info("Purging ALL the databases from all vulnerability entries");
            var metadataUtility = new MetadataUtility();
            metadataUtility.purgeFromDB(contexts);
        }
    }
}
